{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf9cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 13:07:00,187 - ERROR - Error loading data: Data file not found: data\\FAOSTAT_data_en_8-3-2025.csv\n",
      "2025-08-04 13:07:00,188 - ERROR - Error in analysis pipeline: Data file not found: data\\FAOSTAT_data_en_8-3-2025.csv\n",
      "2025-08-04 13:07:00,189 - ERROR - Analysis failed: Data file not found: data\\FAOSTAT_data_en_8-3-2025.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FAO Statistical Data Analysis Pipeline\n",
    "=====================================\n",
    "\n",
    "A comprehensive data analysis pipeline for FAO statistical data including:\n",
    "- Data cleaning and preprocessing\n",
    "- Exploratory data analysis with visualizations\n",
    "- Clustering analysis\n",
    "- Regression modeling\n",
    "\n",
    "Author: Data Analysis Pipeline\n",
    "Version: 2.0\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ML imports\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score, \n",
    "    silhouette_score\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for analysis parameters.\"\"\"\n",
    "    DATA_PATH: str = \"./fao_cleaned_data_20250804_095749.csv\"\n",
    "    TEST_SIZE: float = 0.2\n",
    "    RANDOM_STATE: int = 42\n",
    "    N_CLUSTERS: int = 3\n",
    "    IQR_MULTIPLIER: float = 1.5\n",
    "    FIGURE_SIZE: Tuple[int, int] = (10, 6)\n",
    "    LARGE_FIGURE_SIZE: Tuple[int, int] = (12, 8)\n",
    "    \n",
    "    # Column configurations\n",
    "    REQUIRED_COLUMNS: List[str] = None\n",
    "    TEXT_COLUMNS: List[str] = None\n",
    "    LABEL_ENCODE_COLUMNS: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize column lists after dataclass creation.\"\"\"\n",
    "        self.REQUIRED_COLUMNS = ['Value', 'Area', 'Indicator', 'Year']\n",
    "        self.TEXT_COLUMNS = ['Domain', 'Area', 'Indicator', 'Sex', 'Element', 'Source', 'Unit']\n",
    "        self.LABEL_ENCODE_COLUMNS = ['Domain', 'Area', 'Indicator', 'Sex', 'Element', 'Source', 'Unit']\n",
    "\n",
    "\n",
    "class FAODataAnalyzer:\n",
    "    \"\"\"Main class for FAO data analysis pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        \"\"\"Initialize the analyzer with configuration.\"\"\"\n",
    "        self.config = config\n",
    "        self.df = None\n",
    "        self.encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        \n",
    "        # Set style for all plots\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        plt.style.use('default')\n",
    "    \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load data from CSV file with error handling.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Loaded dataframe\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If data file doesn't exist\n",
    "            pd.errors.EmptyDataError: If file is empty\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_path = Path(self.config.DATA_PATH)\n",
    "            if not data_path.exists():\n",
    "                raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "            \n",
    "            logger.info(f\"Loading data from {data_path}\")\n",
    "            self.df = pd.read_csv(data_path)\n",
    "            logger.info(f\"Data loaded successfully. Shape: {self.df.shape}\")\n",
    "            \n",
    "            # Validate required columns\n",
    "            missing_cols = set(self.config.REQUIRED_COLUMNS) - set(self.df.columns)\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "            \n",
    "            return self.df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def clean_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean and preprocess the dataframe.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Cleaned dataframe\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "        \n",
    "        logger.info(\"Starting data cleaning process\")\n",
    "        initial_shape = self.df.shape\n",
    "        \n",
    "        # Handle missing values\n",
    "        logger.info(\"Handling missing values...\")\n",
    "        missing_summary = self.df.isnull().sum()\n",
    "        logger.info(f\"Missing values summary:\\n{missing_summary[missing_summary > 0]}\")\n",
    "        \n",
    "        # Drop rows with missing critical fields\n",
    "        self.df.dropna(subset=self.config.REQUIRED_COLUMNS, inplace=True)\n",
    "        \n",
    "        # Fill optional text fields\n",
    "        optional_text_fields = ['Note', 'Flag', 'Flag Description']\n",
    "        for field in optional_text_fields:\n",
    "            if field in self.df.columns:\n",
    "                self.df[field] = self.df[field].fillna('')\n",
    "        \n",
    "        # Standardize text columns\n",
    "        logger.info(\"Standardizing text columns...\")\n",
    "        for col in self.config.TEXT_COLUMNS:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].astype(str).str.strip().str.title()\n",
    "        \n",
    "        # Remove outliers using IQR method\n",
    "        logger.info(\"Removing outliers...\")\n",
    "        self.df = self._remove_outliers(self.df, 'Value')\n",
    "        \n",
    "        # Label encoding\n",
    "        logger.info(\"Applying label encoding...\")\n",
    "        self._apply_label_encoding()\n",
    "        \n",
    "        # Scale values\n",
    "        logger.info(\"Scaling numerical values...\")\n",
    "        self.df['Value_Scaled'] = self.scaler.fit_transform(self.df[['Value']])\n",
    "        \n",
    "        final_shape = self.df.shape\n",
    "        logger.info(f\"Data cleaning completed. Shape changed from {initial_shape} to {final_shape}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def _remove_outliers(self, df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "        \"\"\"Remove outliers using IQR method.\"\"\"\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - self.config.IQR_MULTIPLIER * IQR\n",
    "        upper_bound = Q3 + self.config.IQR_MULTIPLIER * IQR\n",
    "        \n",
    "        initial_count = len(df)\n",
    "        df_clean = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "        final_count = len(df_clean)\n",
    "        \n",
    "        logger.info(f\"Removed {initial_count - final_count} outliers from {column}\")\n",
    "        return df_clean\n",
    "    \n",
    "    def _apply_label_encoding(self) -> None:\n",
    "        \"\"\"Apply label encoding to categorical columns.\"\"\"\n",
    "        for col in self.config.LABEL_ENCODE_COLUMNS:\n",
    "            if col in self.df.columns:\n",
    "                le = LabelEncoder()\n",
    "                self.df[f'{col}_Encoded'] = le.fit_transform(self.df[col])\n",
    "                self.encoders[col] = le\n",
    "    \n",
    "    def generate_summary_statistics(self) -> None:\n",
    "        \"\"\"Generate and display summary statistics.\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not available. Load and clean data first.\")\n",
    "        \n",
    "        logger.info(\"Generating summary statistics\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"DATASET SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"Dataset shape: {self.df.shape}\")\n",
    "        print(f\"Memory usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        print(\"DATA TYPES\")\n",
    "        print(\"-\" * 40)\n",
    "        print(self.df.dtypes.value_counts())\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        print(\"NUMERICAL STATISTICS\")\n",
    "        print(\"-\" * 40)\n",
    "        print(self.df.describe())\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        print(\"CATEGORICAL STATISTICS\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Unique Areas: {self.df['Area'].nunique()}\")\n",
    "        print(f\"Unique Indicators: {self.df['Indicator'].nunique()}\")\n",
    "        \n",
    "        if 'Sex' in self.df.columns:\n",
    "            print(f\"\\nSex distribution:\\n{self.df['Sex'].value_counts()}\")\n",
    "        \n",
    "        if 'Element' in self.df.columns:\n",
    "            print(f\"\\nElement distribution:\\n{self.df['Element'].value_counts()}\")\n",
    "    \n",
    "    def create_exploratory_plots(self) -> None:\n",
    "        \"\"\"Create comprehensive exploratory data analysis plots.\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not available. Load and clean data first.\")\n",
    "        \n",
    "        logger.info(\"Creating exploratory plots\")\n",
    "        \n",
    "        # 1. Distribution of Values\n",
    "        plt.figure(figsize=self.config.FIGURE_SIZE)\n",
    "        sns.histplot(self.df['Value'], bins=50, kde=True, alpha=0.7)\n",
    "        plt.title('Distribution of Values', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Boxplot by Element (if available)\n",
    "        if 'Element' in self.df.columns and self.df['Element'].nunique() <= 20:\n",
    "            plt.figure(figsize=self.config.LARGE_FIGURE_SIZE)\n",
    "            sns.boxplot(data=self.df, x='Element', y='Value')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.title('Distribution of Values by Element', fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # 3. Time series analysis\n",
    "        if 'Year' in self.df.columns:\n",
    "            yearly_avg = self.df.groupby('Year')['Value'].agg(['mean', 'std']).reset_index()\n",
    "            \n",
    "            plt.figure(figsize=self.config.FIGURE_SIZE)\n",
    "            plt.plot(yearly_avg['Year'], yearly_avg['mean'], marker='o', linewidth=2)\n",
    "            plt.fill_between(yearly_avg['Year'], \n",
    "                           yearly_avg['mean'] - yearly_avg['std'],\n",
    "                           yearly_avg['mean'] + yearly_avg['std'], \n",
    "                           alpha=0.3)\n",
    "            plt.title('Average Value Over Time (with Standard Deviation)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Year')\n",
    "            plt.ylabel('Average Value')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # 4. Gender comparison (if available)\n",
    "        if 'Sex' in self.df.columns:\n",
    "            sex_data = self.df[self.df['Sex'].isin(['Male', 'Female'])]\n",
    "            if not sex_data.empty:\n",
    "                plt.figure(figsize=self.config.FIGURE_SIZE)\n",
    "                sns.boxplot(data=sex_data, x='Sex', y='Value')\n",
    "                plt.title('Value Distribution by Gender', fontsize=14, fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        # 5. Correlation matrix\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 1:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            correlation_matrix = self.df[numeric_cols].corr()\n",
    "            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', \n",
    "                       center=0, fmt='.2f', square=True)\n",
    "            plt.title('Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def perform_clustering_analysis(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform K-means clustering analysis on areas by average value.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Clustering results and metrics\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not available. Load and clean data first.\")\n",
    "        \n",
    "        logger.info(\"Performing clustering analysis\")\n",
    "        \n",
    "        # Group by area and calculate mean values\n",
    "        df_grouped = self.df.groupby('Area')[['Value']].mean().reset_index()\n",
    "        \n",
    "        # Scale the values for clustering\n",
    "        scaler = StandardScaler()\n",
    "        df_grouped['Value_scaled'] = scaler.fit_transform(df_grouped[['Value']])\n",
    "        \n",
    "        # Perform K-means clustering\n",
    "        kmeans = KMeans(n_clusters=self.config.N_CLUSTERS, \n",
    "                       random_state=self.config.RANDOM_STATE)\n",
    "        df_grouped['Cluster'] = kmeans.fit_predict(df_grouped[['Value_scaled']])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        silhouette_avg = silhouette_score(df_grouped[['Value_scaled']], \n",
    "                                        df_grouped['Cluster'])\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=self.config.LARGE_FIGURE_SIZE)\n",
    "        scatter = sns.scatterplot(data=df_grouped, x='Area', y='Value', \n",
    "                                hue='Cluster', palette='Set2', s=100)\n",
    "        plt.xticks(rotation=90, ha='right')\n",
    "        plt.title(f'K-Means Clustering of Areas by Average Value (k={self.config.N_CLUSTERS})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Country/Area')\n",
    "        plt.ylabel('Average Value')\n",
    "        plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Results summary\n",
    "        results = {\n",
    "            'silhouette_score': round(silhouette_avg, 3),\n",
    "            'inertia': round(kmeans.inertia_, 2),\n",
    "            'n_clusters': self.config.N_CLUSTERS,\n",
    "            'cluster_sizes': df_grouped['Cluster'].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Clustering completed. Silhouette Score: {results['silhouette_score']}\")\n",
    "        return results\n",
    "    \n",
    "    def build_regression_model(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Build and evaluate a Random Forest regression model.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Model performance metrics\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not available. Load and clean data first.\")\n",
    "        \n",
    "        logger.info(\"Building regression model\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare features for modeling\n",
    "            feature_columns = ['Year']\n",
    "            categorical_features = []\n",
    "            \n",
    "            # Add categorical features that exist and have reasonable cardinality\n",
    "            for col in ['Sex', 'Indicator', 'Element']:\n",
    "                if col in self.df.columns and self.df[col].nunique() <= 50:\n",
    "                    categorical_features.append(col)\n",
    "            \n",
    "            feature_columns.extend(categorical_features)\n",
    "            \n",
    "            # Create feature dataframe\n",
    "            df_model = self.df[feature_columns + ['Value']].copy()\n",
    "            \n",
    "            # One-hot encode categorical variables\n",
    "            if categorical_features:\n",
    "                df_encoded = pd.get_dummies(df_model[feature_columns], \n",
    "                                          columns=categorical_features, \n",
    "                                          drop_first=True)\n",
    "            else:\n",
    "                df_encoded = df_model[feature_columns].copy()\n",
    "            \n",
    "            # Prepare features and target\n",
    "            X = df_encoded\n",
    "            y = df_model['Value']\n",
    "            \n",
    "            # Train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.config.TEST_SIZE, \n",
    "                random_state=self.config.RANDOM_STATE\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            self.model = RandomForestRegressor(random_state=self.config.RANDOM_STATE)\n",
    "            self.model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = self.model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'rmse': round(np.sqrt(mean_squared_error(y_test, y_pred)), 2),\n",
    "                'mae': round(mean_absolute_error(y_test, y_pred), 2),\n",
    "                'r2_score': round(r2_score(y_test, y_pred), 3),\n",
    "                'n_features': X.shape[1],\n",
    "                'n_samples': len(X)\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Model training completed. R² Score: {metrics['r2_score']}\")\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model building: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def print_model_results(self, clustering_results: Dict, model_metrics: Dict) -> None:\n",
    "        \"\"\"Print formatted results summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ANALYSIS RESULTS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(\"\\nCLUSTERING ANALYSIS:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Number of clusters: {clustering_results['n_clusters']}\")\n",
    "        print(f\"Silhouette Score: {clustering_results['silhouette_score']}\")\n",
    "        print(f\"Inertia: {clustering_results['inertia']}\")\n",
    "        print(\"Cluster sizes:\", clustering_results['cluster_sizes'])\n",
    "        \n",
    "        print(\"\\nREGRESSION MODEL PERFORMANCE:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"RMSE: {model_metrics['rmse']}\")\n",
    "        print(f\"MAE: {model_metrics['mae']}\")\n",
    "        print(f\"R² Score: {model_metrics['r2_score']}\")\n",
    "        print(f\"Number of features: {model_metrics['n_features']}\")\n",
    "        print(f\"Number of samples: {model_metrics['n_samples']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    def run_complete_analysis(self) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"\n",
    "        Run the complete analysis pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: Clustering results and model metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load and clean data\n",
    "            self.load_data()\n",
    "            self.clean_data()\n",
    "            \n",
    "            # Generate summary statistics\n",
    "            self.generate_summary_statistics()\n",
    "            \n",
    "            # Create exploratory plots\n",
    "            self.create_exploratory_plots()\n",
    "            \n",
    "            # Perform clustering analysis\n",
    "            clustering_results = self.perform_clustering_analysis()\n",
    "            \n",
    "            # Build regression model\n",
    "            model_metrics = self.build_regression_model()\n",
    "            \n",
    "            # Print results summary\n",
    "            self.print_model_results(clustering_results, model_metrics)\n",
    "            \n",
    "            logger.info(\"Complete analysis pipeline finished successfully\")\n",
    "            return clustering_results, model_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in analysis pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis.\"\"\"\n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "    \n",
    "    # Create analyzer instance\n",
    "    analyzer = FAODataAnalyzer(config)\n",
    "    \n",
    "    # Run complete analysis\n",
    "    try:\n",
    "        clustering_results, model_metrics = analyzer.run_complete_analysis()\n",
    "        \n",
    "        # Optional: Save results to file\n",
    "        # results = {\n",
    "        #     'clustering': clustering_results,\n",
    "        #     'regression': model_metrics\n",
    "        # }\n",
    "        # with open('analysis_results.json', 'w') as f:\n",
    "        #     json.dump(results, f, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis failed: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    exit(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
